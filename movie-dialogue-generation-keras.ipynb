{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dialogues_path = \"./data/movie_lines.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = \"~e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dialogue_lines = list()\n",
    "with open(dialogues_path) as dialogues_file:\n",
    "    for line in dialogues_file:\n",
    "        line = line.strip().lower()\n",
    "        split_line = line.split(' +++$+++ ')\n",
    "        dialogue_lines.append(split_line[4] + \" \" + EOS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras_tokenizer.fit_on_texts(dialogue_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(keras_tokenizer.word_index) + 1\n",
    "print(VOCAB_SIZE)\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_sequences = keras_tokenizer.texts_to_sequences(dialogue_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = max(len(sequence) for sequence in text_sequences)\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Input, Dense, RepeatVector, LSTM, Conv1D, Masking, Embedding\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', \n",
    "                        truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_rev = list()\n",
    "for x_vector in x_train:\n",
    "    x_rev_vector = list()\n",
    "    for index in x_vector:\n",
    "        char_vector = np.zeros(VOCAB_SIZE)\n",
    "        char_vector[index] = 1\n",
    "        x_rev_vector.append(char_vector)\n",
    "    x_train_rev.append(np.asarray(x_rev_vector))\n",
    "x_train_rev = np.asarray(x_train_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_seq2seq_model():\n",
    "    main_input = Input(shape=x_train[0].shape, dtype='float32', name='main_input')\n",
    "    print(main_input)\n",
    "\n",
    "    embed_1 = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, \n",
    "                        mask_zero=True, input_length=MAX_SEQUENCE_LENGTH) (main_input)\n",
    "    print(embed_1)\n",
    "\n",
    "    lstm_1 = Bidirectional(LSTM(EMBEDDING_DIM, name='lstm_1'))(embed_1)\n",
    "    print(lstm_1)\n",
    "\n",
    "    repeat_1 = RepeatVector(MAX_SEQUENCE_LENGTH, name='repeat_1')(lstm_1)\n",
    "    print(repeat_1)\n",
    "\n",
    "    lstm_3 = Bidirectional(LSTM(100, return_sequences=True, name='lstm_3'))(repeat_1)\n",
    "    print(lstm_3)\n",
    "\n",
    "    softmax_1 = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))(lstm_3)\n",
    "    print(softmax_1)\n",
    "    \n",
    "    model = Model(main_input, softmax_1)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model = get_seq2seq_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model.fit(x_train, x_train_rev, batch_size=32, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = seq2seq_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2word_map = inv_map = {v: k for k, v in keras_tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_to_str(sequence):\n",
    "    word_list = list()\n",
    "    for element in sequence:\n",
    "#         if amax(element) < max_prob:\n",
    "#             continue\n",
    "        index = np.argmax(element) + 1\n",
    "        word = index2word_map[index]\n",
    "        word_list.append(word)\n",
    "        \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predictions)):\n",
    "    predicted_word_list = sequence_to_str(predictions[i])\n",
    "    actual_len = len(dialogue_lines[i])\n",
    "    print(\"Actual: \" + dialogue_lines[i][:len(dialogue_lines[i])-3])\n",
    "    generated_sentence = \"\"\n",
    "    for word in predicted_word_list:\n",
    "        if word == EOS_TOKEN:\n",
    "            print('\\n')\n",
    "            break\n",
    "        generated_sentence += word + \" \"\n",
    "    print(\"Generated: \" + generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyenv",
   "language": "python",
   "name": ".pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
