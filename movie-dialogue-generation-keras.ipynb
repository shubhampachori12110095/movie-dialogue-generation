{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dialogues_path = \"./data/movie_lines.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = \"~e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_lines = list()\n",
    "with open(dialogues_path) as dialogues_file:\n",
    "    for line in dialogues_file:\n",
    "        line = line.strip().lower()\n",
    "        split_line = line.split(' +++$+++ ')\n",
    "        try:\n",
    "            dialogue_lines.append(split_line[4] + \" \" + EOS_TOKEN)\n",
    "        except IndexError:\n",
    "            pass\n",
    "#             print(\"Skipped line \" + line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they do not! ~e',\n",
       " 'they do to! ~e',\n",
       " 'i hope so. ~e',\n",
       " 'she okay? ~e',\n",
       " \"let's go. ~e\",\n",
       " 'wow ~e',\n",
       " \"okay -- you're gonna need to learn how to lie. ~e\",\n",
       " 'no ~e',\n",
       " 'i\\'m kidding.  you know how sometimes you just become this \"persona\"?  and you don\\'t know how to quit? ~e',\n",
       " 'like my fear of wearing pastels? ~e']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras_tokenizer.fit_on_texts(dialogue_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55857\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(keras_tokenizer.word_index) + 1\n",
    "print(VOCAB_SIZE)\n",
    "EMBEDDING_DIM = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences = keras_tokenizer.texts_to_sequences(dialogue_lines)[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = max(len(sequence) for sequence in text_sequences)\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Input, Dense, RepeatVector, LSTM, Conv1D, Masking, Embedding\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', \n",
    "                        truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 207)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rev = list()\n",
    "for x_vector in x_train:\n",
    "    x_rev_vector = list()\n",
    "    for index in x_vector:\n",
    "        char_vector = np.zeros(VOCAB_SIZE)\n",
    "        char_vector[index] = 1\n",
    "        x_rev_vector.append(char_vector)\n",
    "    x_train_rev.append(np.asarray(x_rev_vector))\n",
    "x_train_rev = np.asarray(x_train_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_seq2seq_model():\n",
    "    main_input = Input(shape=x_train[0].shape, dtype='float32', name='main_input')\n",
    "    print(main_input)\n",
    "\n",
    "    embed_1 = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, \n",
    "                        mask_zero=True, input_length=MAX_SEQUENCE_LENGTH) (main_input)\n",
    "    print(embed_1)\n",
    "\n",
    "    lstm_1 = Bidirectional(LSTM(EMBEDDING_DIM, name='lstm_1'))(embed_1)\n",
    "    print(lstm_1)\n",
    "\n",
    "    repeat_1 = RepeatVector(MAX_SEQUENCE_LENGTH, name='repeat_1')(lstm_1)\n",
    "    print(repeat_1)\n",
    "\n",
    "    lstm_3 = Bidirectional(LSTM(100, return_sequences=True, name='lstm_3'))(repeat_1)\n",
    "    print(lstm_3)\n",
    "\n",
    "    softmax_1 = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))(lstm_3)\n",
    "    print(softmax_1)\n",
    "    \n",
    "    model = Model(main_input, softmax_1)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model = get_seq2seq_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model.fit(x_train, x_train_rev, batch_size=32, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = seq2seq_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2word_map = inv_map = {v: k for k, v in keras_tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_to_str(sequence):\n",
    "    word_list = list()\n",
    "    for element in sequence:\n",
    "#         if amax(element) < max_prob:\n",
    "#             continue\n",
    "        index = np.argmax(element) + 1\n",
    "        word = index2word_map[index]\n",
    "        word_list.append(word)\n",
    "        \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predictions)):\n",
    "    predicted_word_list = sequence_to_str(predictions[i])\n",
    "    actual_len = len(dialogue_lines[i])\n",
    "    print(\"Actual: \" + dialogue_lines[i][:len(dialogue_lines[i])-3])\n",
    "    generated_sentence = \"\"\n",
    "    for word in predicted_word_list:\n",
    "        if word == EOS_TOKEN:\n",
    "            print('\\n')\n",
    "            break\n",
    "        generated_sentence += word + \" \"\n",
    "    print(\"Generated: \" + generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
